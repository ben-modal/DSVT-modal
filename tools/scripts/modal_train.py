# # Training DSVT on NuScenes v1.0-mini with Modal

# ## Setup

from pathlib import Path

import modal


# ### NuScenes Dataset Setup
# To automatically download and setup the v1.0-mini subset of NuScenes you need to make an account here:
#       https://www.nuscenes.org/nuscenes#download
# Then upload your username and password as a [Modal Secret](https://modal.com/secrets/):
# 1) Click the Custom secret button,
# 2) name the secret `nuscenes`,
# 3) then enter two key/value pairs: {NUSCENES_USERNAME: your_email, NUSCENES_PASSWORD: your_password}.
#
# Then, the following line will import these values as environment variables:
nuscenes_secret = modal.Secret.from_name("nuscenes")

# We'll download the data into a Modal.Volume with this name:
vol_name = "example-nuscenes"
# This is the relative location within the container where this Volume will be mounted:
vol_mnt = Path("/data")
vol_data_subdir = "nuscenes"  # data subdir within the volume
config_cache_subdir = "model-checkpoints"  # you can save whatever you want in there


# Initialize the the Volume object (creating one if you haven't already):
nuscenes_volume = modal.Volume.from_name(vol_name, create_if_missing=True)

# ### Define the image
# Here we build all the necessary libraries to run DSVT and OpenPCDet.
# It's somewhat nuanced, with specialized versions of numpy etc., so beware of fiddling.
nuscenes_image = (
    modal.Image.from_registry("nvidia/cuda:11.8.0-devel-ubuntu20.04", add_python="3.9")
    .env(
        {  # Some environment variable needed to compile the libs in the image
            "DEBIAN_FRONTEND": "noninteractive",
            "TORCH_CUDA_ARCH_LIST": "8.0;8.6",
            "CXX": "g++",
            "CC": "gcc",
        }
    )
    .apt_install(["git", "python3-opencv", "build-essential", "ninja-build", "clang"])
    .run_commands("pip install --upgrade pip")
    .pip_install(["uv"])
    .run_commands(
        "uv pip install --system --index-strategy unsafe-best-match "
        "'numpy==1.23.5' 'scikit-image<=0.21.0' "
        # Doesn't have to be this version of Torch, but need to commit to a version and stay consistent.
        "'torch==2.0.1+cu118' 'torchvision==0.15.2+cu118' 'torchaudio==2.0.2+cu118' "
        "--index-url https://download.pytorch.org/whl/cu118 "
        "--extra-index-url https://pypi.org/simple"
    )
    .run_commands(
        "uv pip install --system --no-build-isolation spconv-cu118 torch-scatter "
        "-f https://data.pyg.org/whl/torch-2.0.1+cu118.html"
    )
    .run_commands(
        "uv pip install --system tensorrt onnx pyyaml 'nuscenes-devkit==1.0.5'"
    )
    # Here we clone some repos and build them. NOTE: you could instead import your a local
    # version of the repo using Modal's `add_local_dir`:
    # https://modal.com/docs/guide/images#add-local-files-with-add_local_dir-and-add_local_file
    #
    .run_commands("git clone https://github.com/beijbom/DSVT.git")
    .run_commands("uv pip install --system --no-build-isolation -e DSVT")
    .run_commands("uv pip install --system 'mmcv>=1.4.0,<2.0.0'")
    # NOTE: it might be possible to use DSVT's internally copied subset of pcdet,
    # which would save time as this takes a while:
    .run_commands("git clone https://github.com/open-mmlab/OpenPCDet.git")
    .run_commands("uv pip install --system --no-build-isolation -e OpenPCDet")
    .run_commands("uv pip install --system 'av2==0.2.0' 'kornia<0.7'")
    .entrypoint([])
)

# Initialize the app
app = modal.App(
    "train-nuscenes",
    image=nuscenes_image,
    volumes={vol_mnt: nuscenes_volume},
)


# ## NuScenes Automated Downloading + Preprocessing
# The function `download_nuscenes` automatically downloads and preprocesses the NuScenes dataset.
# Tested with the v1.0-mini partition only -- the massive v1.0-train dataset may need special
# handling (for example, distributing across several Modal Volumes).
# NOTE: we cap max_containers with 1 here, but you could distribute the download
# and preprocessing over subsets of v1.0-train and v1.0-test.
# NOTE: it's unclear whether a GPU is necessary for preprocessing, but `mmcv` throws a lot
# of warnings if one is not available.
@app.function(
    image=nuscenes_image,
    secrets=[nuscenes_secret],
    volumes={vol_mnt: nuscenes_volume},
    timeout=60 * 60,  # preprocessing v1.0-mini from scratch takes 30-60min
    gpu="T4",
    max_containers=1,
)
def download_nuscenes(
    volume_subdir: str,
    region: str = "us",  # or "asia"
    dataset_version: str = "v1.0-mini",  # only tested with v1.0-mini
):
    """
    Inputs:
        - volume_subdir: subdir within the volume where all this data should be downloaded/extracted
        - region: 'us' or 'asia'
        - dataset_version: string identifying which subset of the NuScenes dataset to download, see
            https://www.nuscenes.org/nuscenes#download

    Automated download code inspired by:
    https://github.com/li-xl/nuscenes-download/blob/master/download_nuscenes.py
    """
    import json
    import os
    import subprocess
    import sys
    import tarfile

    import requests
    from tqdm import tqdm

    download_dir = vol_mnt / volume_subdir
    tgz_file = download_dir / f"{dataset_version}.tgz"
    extract_dir = download_dir / dataset_version
    info_prefix = "nuscenes"

    # (0) Download .tgz from AWS:
    if not tgz_file.is_file():
        download_dir.mkdir(parents=True, exist_ok=True)
        # (1a) Get login from Modal Secret
        if not os.getenv("NUSCENES_USERNAME") or not os.getenv("NUSCENES_PASSWORD"):
            print(
                "Error: set NUSCENES_USERNAME and NUSCENES_PASSWORD in your env",
                file=sys.stderr,
            )
            sys.exit(1)

        # (1b) Log in via Cognito to get a bearer token
        print(f"Setting up Nuscenes mini dataset in `{vol_name}/{volume_subdir}`.")
        resp = requests.post(
            "https://cognito-idp.us-east-1.amazonaws.com/",
            headers={
                "Content-Type": "application/x-amz-json-1.1",
                "X-Amz-Target": "AWSCognitoIdentityProviderService.InitiateAuth",
            },
            data=json.dumps(
                {
                    "AuthFlow": "USER_PASSWORD_AUTH",
                    "ClientId": "7fq5jvs5ffs1c50hd3toobb3b9",
                    "AuthParameters": {
                        "USERNAME": os.getenv("NUSCENES_USERNAME"),
                        "PASSWORD": os.getenv("NUSCENES_PASSWORD"),
                    },
                }
            ),
        )
        resp.raise_for_status()
        token = resp.json()["AuthenticationResult"]["IdToken"]
        print("\tLogged in successfully")

        # (1c) Fetch the mini archive URL
        api = (
            f"https://o9k5xn5546.execute-api.us-east-1.amazonaws.com/v1"
            f"/archives/v1.0/{tgz_file.name}?region={region}&project=nuScenes"
        )
        resp = requests.get(api, headers={"Authorization": f"Bearer {token}"})
        resp.raise_for_status()
        download_url = resp.json()["url"]
        print(f"\tGot download URL for {tgz_file.name}")

        # (1d) Download into download_dir
        os.makedirs(download_dir, exist_ok=True)

        with requests.get(download_url, stream=True) as r:
            r.raise_for_status()
            total = int(r.headers.get("Content-Length", 0))
            with (
                open(tgz_file, "wb") as f,
                tqdm(total=total, unit="B", unit_scale=True, desc="Downloading") as bar,
            ):
                for chunk in r.iter_content(chunk_size=8192):
                    if not chunk:
                        continue
                    f.write(chunk)
                    bar.update(len(chunk))
        print(f"\tDownloaded to {tgz_file}")
    else:
        print(f"\t.tgz archive found at: {tgz_file}")

    # (2) Extract the archive in-place
    n_files = len(list(extract_dir.glob("*")))
    if n_files < 3:
        print(f"\tExtracting to {extract_dir}")
        with tarfile.open(tgz_file, "r:gz") as tar:
            for member in tqdm(tar.getmembers(), desc="Extracting", unit="file"):
                tar.extract(member, path=extract_dir)  # NOW extract
        print("\tExtraction complete")
    else:
        print(
            f"\t{n_files} found at {extract_dir}, assuming archive already extracted...!"
        )

    # (3) Generate metadata with nuScenes devkit
    pickles = [
        extract_dir / f"{info_prefix}_infos_10sweeps_train.pkl",
        extract_dir / f"{info_prefix}_dbinfos_10sweeps_withvelo.pkl",
        extract_dir / f"{info_prefix}_infos_10sweeps_val.pkl",
    ]
    if all([x.is_file() for x in pickles]):
        print("\tpickle files found!")
    else:
        print("\tGenerating devkit metadata pickles...")
        os.chdir("/OpenPCDet")  # TODO: try using DSVT's copy inside its repo..
        cmd = [
            sys.executable,
            "-m",
            "pcdet.datasets.nuscenes.nuscenes_dataset",
            "--func",
            "create_nuscenes_infos",
            "--cfg_file",
            "tools/cfgs/dataset_configs/nuscenes_dataset.yaml",
            "--version",
            f"{dataset_version}",
            "--with_cam",
        ]
        subprocess.run(cmd, check=True)

    print(f"\tNuScenes {dataset_version} is ready in: {extract_dir}")


# ## Trainer application
# This class encapsulates the call to `train.py`. It could easily be collapsed into a simpler function,
# but we set it up as a class to demonstrate the `with_options` constructor decorator (see function below),
# which allows us to dynamically configure GPU type and count.
#
# The `train` method is the important one: this constructs and executes a call to train.py.
# The `default_nuscenes_config_setup` shows how to programmatically modify the model and data
# config so as to point at the data volume and each other.
@app.cls(
    image=nuscenes_image,
    volumes={vol_mnt: nuscenes_volume},
    timeout=24 * 60 * 60,
    max_containers=1,
)
class DSVTTrainer:
    @modal.method()
    def default_nuscenes_config_setup(
        self,
        tag: str = "",
        data_ver: str = "v1.0-mini",
        model_name: str = "dsvt_plain_1f_onestage_nusences",
        data_name: str = "nuscenes_dataset",
        config_save_dir="saved-configs",
    ):
        """
        This function loads template model/data configs and modifies them to point at the data
        we created in the Modal Volume. This is meant as a placeholder depending on how
        you want to create and save configs.

        Inputs:
            tag: string added to modified configs
            data_ver: string specifying the NuScenes dataset subset version
            model_name: name of a YAML file in DSVT/tools/cfgs/dsvt_models
            data_name: name of a YAML file in DSVT/tools/cfgs/dataset_configs
            config_save_dir: place to save the custom configs
        """
        # Data catalogs
        from yaml import safe_dump, safe_load

        # Directories
        tools = Path("/DSVT") / "tools"
        model_configs = tools / "cfgs/dsvt_models"
        data_configs = tools / "cfgs/dataset_configs"

        # Template config defaults:
        template_model_path = model_configs / f"{model_name}.yaml"
        template_data_path = data_configs / f"{data_name}.yaml"

        # Configs we'll create & use
        savedir = vol_mnt / config_save_dir
        savedir.mkdir(exist_ok=True, parents=True)
        output_data_path = savedir / f"{tag}-data-config.yaml"
        output_model_path = savedir / f"{tag}-model-config.yaml"

        # Data catalogs
        data_dir = vol_mnt / vol_data_subdir / data_ver
        train_data = data_dir / "nuscenes_infos_10sweeps_train.pkl"
        val_data = data_dir / "nuscenes_infos_10sweeps_val.pkl"
        velo_data = data_dir / "nuscenes_dbinfos_10sweeps_withvelo.pkl"

        ########################################################
        # (1) Edit the data config

        with open(template_data_path, "r") as f:
            data_config = safe_load(f)

        data_config["DATA_PATH"] = (vol_mnt / vol_data_subdir).as_posix()
        data_config["VERSION"] = data_ver
        data_config["INFO_PATH"] = {
            "train": [train_data.as_posix()],
            "test": [val_data.as_posix()],
        }

        data_config["DATA_AUGMENTOR"]["AUG_CONFIG_LIST"][0]["DB_INFO_PATH"] = [
            velo_data.as_posix()
        ]

        with open(output_data_path, "w") as f:
            safe_dump(data_config, f)

        ########################################################
        # (2) Edit model config

        with open(template_model_path, "r") as f:
            model_config = safe_load(f)
        model_config["DATA_CONFIG"]["DATA_AUGMENTOR"]["AUG_CONFIG_LIST"][0][
            "DB_INFO_PATH"
        ] = [velo_data.as_posix()]
        # Point model config to our new data config
        model_config["DATA_CONFIG"]["_BASE_CONFIG_"] = output_data_path.as_posix()

        with open(output_model_path, "w") as f:
            safe_dump(model_config, f)

        print(
            f"Using modified version of configs:"
            f"\n\tmodel: {template_model_path}"
            f"\n\tdata: {template_data_path}"
            f"See modified versions at: {savedir}"
        )
        return output_model_path.as_posix()

    @modal.method()
    def train(
        self,
        model_config_path: str,
        exp_name: str = "",
        params: dict = {},
    ):
        """
        Method that creates a command for executing train.py.
        Inputs:
            model_config_path: path to a config file (mandatory input to train.py)
            exp_name: optional tag for printing
            params: dict of flags passed on to train.py
        """
        import os

        import torch

        # Prepare inputs
        flags = " ".join([f"--{arg}={val}" for arg, val in params.items()])
        cmd = (
            f"torchrun "
            f"--nproc_per_node={torch.cuda.device_count()} "
            f"/DSVT/tools/train.py --launcher none "
            f"--cfg_file {model_config_path} " + flags
        )
        if exp_name:
            print(f"Running exp {exp_name} with command:\n\t{cmd}")

        # Execute
        os.chdir("/DSVT/tools")
        os.system(cmd)


@app.local_entrypoint()
def main(gpu: str = "A100", data_ver: str = "v1.0-mini"):
    # TODO: currently getting OOM if n_gpus > 1
    n_gpus = 1

    # (0) Check for data before firing up the downloader/preprocessor container
    # Check if the necessary pickle files exist:
    pickles = [
        "nuscenes_infos_10sweeps_train.pkl",
        "nuscenes_infos_10sweeps_val.pkl",
        "nuscenes_dbinfos_10sweeps_withvelo.pkl",
    ]

    run_downloader = False
    try:
        paths = [
            Path(x.path).name
            for x in nuscenes_volume.listdir(f"{vol_data_subdir}/{data_ver}")
            if x.path.endswith("pkl")
        ]

        for p in pickles:
            if p not in paths:
                run_downloader = True
    except Exception as err:
        # Error if listdir called on non-existent vol
        run_downloader = True

    if run_downloader:
        download_nuscenes.remote(vol_data_subdir)
    else:
        print("Dataset pickles found, skipping download etc.")

    # (1) Data identified! Create the trainer.
    trainer = DSVTTrainer.with_options(gpu=f"{gpu}:{n_gpus}")()

    # Replace this with your custom config setup:
    exp_name = "single-gpu-demo"
    exp_config = trainer.default_nuscenes_config_setup.remote(
        tag=exp_name, data_ver=data_ver
    )
    params = {"epochs": 1, "local_rank": n_gpus}
    # Call train.py (spawns one container)
    trainer.train.remote(exp_name=exp_name, model_config_path=exp_config, params=params)
